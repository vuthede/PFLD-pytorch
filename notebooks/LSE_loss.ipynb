{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from abc import ABCMeta, abstractmethod, ABC\n",
    "\n",
    "\n",
    "\n",
    "class LandmarkDetectorAbstract(ABC):\n",
    "    @abstractmethod\n",
    "    def get_68_landmarks(image):\n",
    "        \"\"\"\n",
    "        In here you have to implement everything you need to return 68 landmark coordimation\n",
    "        given an image. Including face detector + landmark detector\n",
    "        At the end of this method, we will get 2D numpy array with len==68\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You have to implement this method. \\\n",
    "                                   Input is image, output are 2D numpy array representing coordination of landmarks \")\n",
    "\n",
    "\n",
    "class PFLDLandmarkDetector(LandmarkDetectorAbstract):\n",
    "    import dlib\n",
    "    from imutils import face_utils\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"/home/vuthede/VinAI/mydeformation/model.dat\")\n",
    "\n",
    "    def get_rect_and_keypoints(image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        kps_list = []\n",
    "        for rect in rects:\n",
    "            kps = predictor(gray, rect)\n",
    "            kps = face_utils.shape_to_np(kps)\n",
    "            kps_list.append(kps)\n",
    "        \n",
    "        if len(kps_list):\n",
    "            return kps_list[0]\n",
    "    \n",
    "        return []\n",
    "\n",
    "    def get_68_landmarks(self, image):\n",
    "        return get_rect_and_keypoints(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LSE frame 0: 0.0\nLSE frame 1: 0.15632874756086546\nLSE frame 2: 0.12572033832431345\nLSE frame 3: 0.11924433469623062\nLSE frame 4: 0.04220068994452393\nLSE frame 5: 0.10704500080761749\nLSE frame 6: 0.047332489832071736\nLSE frame 7: 0.273954910657909\nLSE frame 8: 0.2494086175462714\nLSE frame 9: 0.3346240394465178\nLSE frame 10: 0.787426366478185\nLSE frame 11: 0.7071279132879125\nLSE frame 12: 0.10235348534327308\nLSE frame 13: 0.2823417810422266\nLSE frame 14: 0.29605979336852156\nLSE frame 15: 0.30469564866130167\nLSE frame 16: 0.24246243041132667\nLSE frame 17: 0.34493080789321845\nLSE frame 18: 0.12709722199935847\nLSE frame 19: 0.13582506320727616\nLSE frame 20: 0.04460755611466434\nLSE frame 21: 0.09572130734861813\nLSE frame 22: 0.28577374214029533\nLSE frame 23: 0.04395814078304099\nLSE frame 24: 0.07546223840712528\nLSE frame 25: 0.20718549646293857\nLSE frame 26: 0.08143050931356569\nLSE frame 27: 0.37219435454028554\nLSE frame 28: 0.3406964015628131\nLSE frame 29: 0.08981059143657331\nLSE frame 30: 0.4760825559648032\nLSE frame 31: 0.4129075672509002\nLSE frame 32: 0.29851743183580837\nLSE frame 33: 0.22893132195741864\nLSE frame 34: 0.09425407900579227\nLSE frame 35: 0.033036154710639205\nLSE frame 36: 0.08905104584181886\nLSE frame 37: 0.19836704014409198\nLSE frame 38: 0.22400060309091163\nLSE frame 39: 0.24515284642035828\nLSE frame 40: 0.03614796179064068\nLSE frame 41: 0.05792133493352283\nLSE frame 42: 0.13187154559729186\nLSE frame 43: 0.07009850490538971\nLSE frame 44: 0.1006078976014748\nLSE frame 45: 0.13721724977257727\nLSE frame 46: 0.2994621726086626\nLSE frame 47: 0.3016583917247846\nLSE frame 48: 0.11542118394676047\nLSE frame 49: 0.16153217565248537\nLSE frame 50: 0.1860713397011152\nLSE frame 51: 0.17090033227639093\nLSE frame 52: 0.08318137035697296\nLSE frame 53: 0.08094767833164818\nLSE frame 54: 0.08732170827353214\nLSE frame 55: 0.066547541949747\nLSE frame 56: 0.17079429875718963\nLSE frame 57: 0.040927411700878734\nLSE frame 58: 0.18723151595581797\nLSE frame 59: 0.4251488753274754\nLSE frame 60: 0.287058241714425\nLSE frame 61: 0.145519197906308\nLSE frame 62: 0.1852556462457399\nLSE frame 63: 0.09903638686117366\nLSE frame 64: 0.0384677032339091\nLSE frame 65: 0.15674559762481186\nLSE frame 66: 0.04687231954489461\nLSE frame 67: 0.16575023085836804\nLSE frame 68: 0.3396306235231529\nLSE frame 69: 0.28372751312261624\nLSE frame 70: 0.10574057171238435\nLSE frame 71: 0.19774004836301842\n"
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateLSEInOneVideo(lmdetector, videopath, annodir):\n",
    "    def get_gt_landmark_from_file(anno):\n",
    "        file1 = open(anno, 'r') \n",
    "        ls = file1.readlines() \n",
    "        ls = ls[3:-1] # Get onlu lines that contain landmarks. 68 lines\n",
    "\n",
    "        lm = []\n",
    "        for l in ls:\n",
    "            l = l.replace(\"\\n\",\"\")\n",
    "            a = l.split(\" \")\n",
    "            a = [float(i) for i in a]\n",
    "            lm.append(a)\n",
    "        \n",
    "        lm = np.array(lm)\n",
    "        assert len(lm)==68, \"There should be 68 landmarks. Get {len(lm)}\"\n",
    "        return lm\n",
    "\n",
    "    anno_files = glob.glob(annodir + \"/*.pts\")\n",
    "    cap = cv2.VideoCapture(videopath)\n",
    "    num_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    assert len(anno_files) == num_frame, f\"Number of annotation files {len(anno_files)} \\\n",
    "                                         is not equal to number of frames {num_frame} \"\n",
    "    \n",
    "    lse_list = [] # List losses in all frames\n",
    "    pre_gt_landmark = None\n",
    "    pre_pred_landmark  = None\n",
    "    for i, anno in enumerate(anno_files):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "    \n",
    "        gt_landmark = get_gt_landmark_from_file(anno)\n",
    "        pred_landmark = lmdetector.get_68_landmarks(frame)\n",
    "\n",
    "        assert gt_landmark.shape == pred_landmark.shape, f\"Shape of pred landmark is \\\n",
    "                                                            different from gt landmark {gt_landmark.shape}\"\n",
    "        \n",
    "        # Calculate LSE for this frame\n",
    "        N=68\n",
    "        interocular = np.linalg.norm(gt_landmark[36] - gt_landmark[45])\n",
    "        if i==0: # The first frame\n",
    "            sum_delta = 0\n",
    "        else:\n",
    "            sum_delta = np.sum(np.linalg.norm((gt_landmark-pre_gt_landmark) - (pred_landmark-pre_pred_landmark), axis=1))\n",
    "        lse_one_frame = sum_delta/(interocular*N)\n",
    "        lse_list.append(lse_one_frame)\n",
    "\n",
    "        print(f\"LSE frame {i}: {lse_one_frame}\")\n",
    "\n",
    "        # Cache the precious predicted and gt landmark for later use in the next frame\n",
    "        pre_gt_landmark = gt_landmark\n",
    "        pre_pred_landmark = pred_landmark\n",
    "\n",
    "\n",
    "    lse_video = sum(lse_list)/len(lse_list)\n",
    "\n",
    "    return lse_video\n",
    "\n",
    "\n",
    "pdld_lm_detector =  PFLDLandmarkDetector()\n",
    "video1 = \"/hdd/data/VinAI/300VW_Dataset_2015_12_14/007/vid.avi\"\n",
    "anno1 = \"/hdd/data/VinAI/300VW_Dataset_2015_12_14/007/annot\"\n",
    "lse = calculateLSEInOneVideo(pdld_lm_detector, videopath=video1, annodir=anno1)\n",
    "print(\"LSE error: \", lse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2.82842712, 1.41421356, 0.        ])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "\n",
    "a = np.array([[1,1],[2,2],[3,3]])\n",
    "b = np.array([[3,3],[3,3],[3,3]])\n",
    "# a-b\n",
    "np.linalg.norm(a-b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[741.178 194.555]\n [742.129 212.826]\n [745.478 230.57 ]\n [750.403 248.113]\n [757.5   263.799]\n [768.059 277.73 ]\n [780.72  289.206]\n [794.731 298.483]\n [810.153 301.851]\n [826.669 299.532]\n [842.467 289.884]\n [856.362 277.209]\n [866.998 261.862]\n [873.249 244.143]\n [876.722 225.768]\n [878.995 206.899]\n [879.264 187.83 ]\n [750.478 173.087]\n [758.796 167.175]\n [769.46  166.239]\n [780.359 168.875]\n [790.426 173.932]\n [815.385 174.126]\n [826.828 169.674]\n [838.791 167.536]\n [850.519 169.543]\n [859.711 175.768]\n [803.612 192.672]\n [803.972 205.263]\n [804.145 217.782]\n [804.469 230.466]\n [793.658 239.254]\n [799.224 241.462]\n [805.13  242.954]\n [811.065 241.72 ]\n [816.778 240.212]\n [763.832 192.615]\n [770.783 188.75 ]\n [779.952 189.178]\n [787.906 195.37 ]\n [779.458 197.647]\n [770.294 197.474]\n [821.803 196.244]\n [829.284 190.377]\n [838.73  190.538]\n [846.55  194.646]\n [839.567 198.979]\n [830.165 199.056]\n [785.014 263.   ]\n [792.308 257.522]\n [799.939 254.421]\n [805.419 256.344]\n [811.039 254.951]\n [819.829 258.588]\n [828.377 263.848]\n [820.253 270.838]\n [811.971 273.674]\n [805.799 274.007]\n [799.697 272.896]\n [792.341 269.418]\n [789.208 263.147]\n [800.139 262.533]\n [805.628 263.204]\n [811.307 263.084]\n [823.778 263.856]\n [811.084 262.318]\n [805.398 262.477]\n [799.893 261.66 ]]\n"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "file1 = open(anno1+\"/000001.pts\", 'r') \n",
    "ls = file1.readlines() \n",
    "lm = []\n",
    "ls = ls[3:-1]\n",
    "for l in ls:\n",
    "    l = l.replace(\"\\n\",\"\")\n",
    "    a = l.split(\" \")\n",
    "    a = [float(i) for i in a]\n",
    "    lm.append(a)\n",
    "print(np.array(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([2.4212, 2.3312, 2.4171])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "def wing_loss(y_true, y_pred, w=10.0, epsilon=2.0, N_LANDMARK = 106):\n",
    "    y_pred = y_pred.reshape(-1, N_LANDMARK, 2)\n",
    "    y_true = y_true.reshape(-1, N_LANDMARK, 2) \n",
    "    \n",
    "    x = y_true - y_pred\n",
    "    c = w * (1.0 - math.log(1.0 + w / epsilon))\n",
    "    absolute_x = torch.abs(x)\n",
    "    losses = torch.where(w > absolute_x, w * torch.log(1.0 + absolute_x/epsilon), absolute_x - c)\n",
    "    loss = torch.mean(torch.sum(losses, axis=[1, 2]), axis=0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_wing_loss(y_true, y_pred, w=10.0, epsilon=2.0, N_LANDMARK = 98):\n",
    "    c = w * (1.0 - math.log(1.0 + w / epsilon))\n",
    "    x = y1 - y\n",
    "    magnitude_x = torch.Tensor(np.linalg.norm(x, axis=2))\n",
    "\n",
    "    losses = torch.where(w > magnitude_x, w * torch.log(1.0 + magnitude_x/epsilon), magnitude_x - c)\n",
    "    losses = torch.mean(losses, axis=1)\n",
    "\n",
    "    return losses # Mean wingloss for each sample in batch\n",
    "\n",
    "\n",
    "y = torch.rand((3, 98,2))\n",
    "y1 = torch.rand((3, 98,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitentranceconda9b4673bfa1754abd8ca93decff70e970",
   "display_name": "Python 3.7.0 64-bit ('entrance': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}